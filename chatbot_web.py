from flask import Flask, render_template, request, jsonify
import os
import sys
import json
from datetime import datetime
import numpy as np
import asyncio
import tiktoken
import platform
from lightrag import QueryParam
from security_middleware import SecurityMiddleware, validate_input, require_api_key, log_security_event

# æ£€æµ‹Pythonç‰ˆæœ¬
PYTHON_VERSION = platform.python_version()
print(f"Pythonç‰ˆæœ¬: {PYTHON_VERSION}")

from lightrag import LightRAG
from lightrag.utils import EmbeddingFunc

# å…¼å®¹æ€§å¯¼å…¥ - å¤„ç†ä¸åŒç‰ˆæœ¬çš„lightrag
try:
    from lightrag.llm import openai_complete_if_cache, openai_embedding
except ImportError:
    # å¦‚æœæ–°ç‰ˆæœ¬ä¸­æ²¡æœ‰è¿™äº›å‡½æ•°ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
    from lightrag.llm import openai_complete, openai_embedding
    openai_complete_if_cache = openai_complete

app = Flask(__name__)

# åˆå§‹åŒ–å®‰å…¨ä¸­é—´ä»¶
security = SecurityMiddleware(app)
security.init_app(app)

# å…¨å±€å˜é‡
rag = None
token_encoder = None
cost_stats = {
    "total_input_tokens": 0,
    "total_output_tokens": 0,
    "total_embedding_tokens": 0,
    "total_cost": 0.0
}
query_history = []
token_usage_history = []  # æ–°å¢ï¼štokenä½¿ç”¨å†å²è®°å½•åˆ—è¡¨

# æˆæœ¬ä¼°ç®—é…ç½®
COST_CONFIG = {
    "gpt-4o-mini": {
        "input_cost_per_1k_tokens": 0.00015,  # $0.00015 per 1K input tokens
        "output_cost_per_1k_tokens": 0.0006,  # $0.0006 per 1K output tokens
    },
    "text-embedding-ada-002": {
        "cost_per_1k_tokens": 0.0001,  # $0.0001 per 1K tokens
    }
}

# è¯„åˆ†ç³»ç»Ÿé…ç½®
SCORING_CONFIG = {
    "comprehensiveness_weight": 0.4,
    "diversity_weight": 0.3,
    "empowerment_weight": 0.3,
    "max_score": 10.0
}

def initialize_rag():
    """åˆå§‹åŒ– LightRAG"""
    global rag, token_encoder
    
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­çš„API Key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚è¯·åœ¨éƒ¨ç½²æ—¶è®¾ç½®æ­¤ç¯å¢ƒå˜é‡ã€‚")
        
        print(f"âœ… APIå¯†é’¥å·²è®¾ç½®")
        
        # åˆå§‹åŒ– token ç¼–ç å™¨
        token_encoder = tiktoken.encoding_for_model("gpt-4o-mini")
        print(f"âœ… Tokenç¼–ç å™¨åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–é”™è¯¯: {e}")
        raise
    
    # å®šä¹‰LLMå’Œembeddingå‡½æ•°
    async def llm_model_func(
        prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
    ) -> str:
        try:
            return await openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=os.getenv("OPENAI_API_KEY"),
                **kwargs
            )
        except Exception as e:
            print(f"LLMè°ƒç”¨é”™è¯¯: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆ
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜: {str(e)}"

    async def embedding_func(texts: list[str]) -> np.ndarray:
        try:
            return await openai_embedding(
                texts,
                model="text-embedding-ada-002",
                api_key=os.getenv("OPENAI_API_KEY")
            )
        except Exception as e:
            print(f"Embeddingè°ƒç”¨é”™è¯¯: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºå¤‡ç”¨
            return np.zeros((len(texts), 1536))

    # åˆå§‹åŒ–LightRAGï¼Œä½¿ç”¨ä¸æˆåŠŸä»£ç ç›¸åŒçš„é…ç½®
    rag = LightRAG(
        working_dir="./stakeholder_management_rag_sync",
        llm_model_func=llm_model_func,
        embedding_func=EmbeddingFunc(
            embedding_dim=1536,
            max_token_size=8192,
            func=embedding_func,
        ),
        addon_params={
            "insert_batch_size": 4,
            "language": "Simplified Chinese",
            "entity_types": ["organization", "person", "geo", "event", "project"],
            "example_number": 3
        },
        enable_llm_cache=True,
        enable_llm_cache_for_entity_extract=True
    )
    
    print("âœ… LightRAG åˆå§‹åŒ–å®Œæˆ")

def detect_language(text):
    """ç®€å•çš„ä¸­è‹±æ–‡æ£€æµ‹"""
    chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
    return 'chinese' if chinese_chars > len(text) * 0.3 else 'english'

def generate_system_prompt(question, language='english'):
    """ç”Ÿæˆæ™ºèƒ½ç³»ç»Ÿæç¤ºè¯"""
    if language == 'chinese':
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ©ç›Šç›¸å…³è€…ç®¡ç†é¡¾é—®ã€‚åŸºäºæä¾›çš„æ–‡æ¡£ä¿¡æ¯ï¼Œè¯·è¯šå®ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. åªåŸºäºæ–‡æ¡£ä¸­çš„ä¿¡æ¯å›ç­”ï¼Œå¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜
2. æä¾›ç»“æ„åŒ–çš„å›ç­”ï¼Œä½¿ç”¨è¦ç‚¹å’Œå­è¦ç‚¹
3. å¦‚æœæ¶‰åŠæ•°æ®æˆ–äº‹å®ï¼Œè¯·å¼•ç”¨å…·ä½“æ¥æº
4. å¯¹äºè¯„ä¼°ç±»é—®é¢˜ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¸»è§‚è¯„ä»·ä¿¡æ¯ï¼Œè¯·è¯´æ˜ä¿¡æ¯ä¸è¶³
5. ä¿æŒä¸“ä¸šã€å®¢è§‚çš„è¯­æ°”

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºä»¥ä¸Šè¦æ±‚å›ç­”ï¼š"""
    else:
        return f"""You are a professional stakeholder management consultant. Based on the provided document information, please answer user questions honestly and accurately.

Answer requirements:
1. Only answer based on information in the documents, if information is insufficient, clearly state this
2. Provide structured answers using bullet points and sub-points
3. If involving data or facts, cite specific sources
4. For evaluation questions, if documents lack sufficient subjective evaluation information, state insufficient information
5. Maintain professional and objective tone

User question: {question}

Please answer based on the above requirements:"""

def calculate_tokens(text):
    """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    return len(token_encoder.encode(text))

def calculate_cost(input_tokens, output_tokens, embedding_tokens=0):
    """è®¡ç®—APIè°ƒç”¨æˆæœ¬"""
    # è®¡ç®—LLMæˆæœ¬
    llm_input_cost = (input_tokens / 1000) * COST_CONFIG["gpt-4o-mini"]["input_cost_per_1k_tokens"]
    llm_output_cost = (output_tokens / 1000) * COST_CONFIG["gpt-4o-mini"]["output_cost_per_1k_tokens"]
    
    # è®¡ç®—embeddingæˆæœ¬
    embedding_cost = (embedding_tokens / 1000) * COST_CONFIG["text-embedding-ada-002"]["cost_per_1k_tokens"]
    
    total_cost = llm_input_cost + llm_output_cost + embedding_cost
    
    return {
        "llm_input_cost": llm_input_cost,
        "llm_output_cost": llm_output_cost,
        "embedding_cost": embedding_cost,
        "total_cost": total_cost
    }

def score_response(query, response, mode):
    """è¯„åˆ†ç³»ç»Ÿï¼šåŸºäºåŸå§‹æ–‡ä»¶çš„è¯„ä¼°æ ‡å‡†"""
    scores = {
        "comprehensiveness": 0.0,
        "diversity": 0.0,
        "empowerment": 0.0
    }
    
    # æ£€æµ‹é€šç”¨é—®é¢˜ç±»å‹
    general_questions = [
        "hi", "hello", "hey", "ä½ å¥½", "æ‚¨å¥½",
        "who are you", "what are you", "ä½ æ˜¯è°", "ä½ æ˜¯ä»€ä¹ˆ",
        "how are you", "ä½ å¥½å—", "ä½ å¥½å—ï¼Ÿ",
        "thanks", "thank you", "è°¢è°¢", "è°¢è°¢æ‚¨",
        "bye", "goodbye", "å†è§", "æ‹œæ‹œ"
    ]
    
    query_lower = query.lower().strip()
    is_general_question = any(gq in query_lower for gq in general_questions)
    
    # è®¡ç®—comprehensivenessï¼ˆå®Œæ•´æ€§ï¼‰
    response_length = len(response)
    query_complexity = len(query.split())
    
    if is_general_question:
        # å¯¹äºé€šç”¨é—®é¢˜ï¼Œåªè¦ä¸æ˜¯"Insufficient Data"å°±ç»™é«˜åˆ†
        if "ä¿¡æ¯ä¸è¶³" not in response and "Insufficient Data" not in response:
            scores["comprehensiveness"] = 8.0
        else:
            # å¦‚æœæ˜¯é€šç”¨é—®é¢˜ä½†è¿”å›äº†Insufficient Dataï¼Œç»™è¾ƒä½åˆ†
            scores["comprehensiveness"] = 3.0
    else:
        # å¯¹äºé¡¹ç›®ç›¸å…³é—®é¢˜ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
        if response_length > 100 and "ä¿¡æ¯ä¸è¶³" not in response and "Insufficient Data" not in response:
            scores["comprehensiveness"] = min(10.0, response_length / 50)
        else:
            scores["comprehensiveness"] = max(1.0, response_length / 20)
    
    # è®¡ç®—diversityï¼ˆå¤šæ ·æ€§ï¼‰
    unique_words = len(set(response.lower().split()))
    total_words = len(response.split())
    if total_words > 0:
        diversity_ratio = unique_words / total_words
        scores["diversity"] = min(10.0, diversity_ratio * 15)
    
    # è®¡ç®—empowermentï¼ˆå¯å‘æ€§ï¼‰
    empowerment_keywords = ["å»ºè®®", "æ¨è", "è€ƒè™‘", "åˆ†æ", "è¯„ä¼°", "å»ºè®®", "recommend", "consider", "analyze", "evaluate"]
    empowerment_count = sum(1 for keyword in empowerment_keywords if keyword.lower() in response.lower())
    scores["empowerment"] = min(10.0, empowerment_count * 2)
    
    # å¯¹äºé€šç”¨é—®é¢˜ï¼Œå¢åŠ empowermentåˆ†æ•°
    if is_general_question and "ä¿¡æ¯ä¸è¶³" not in response and "Insufficient Data" not in response:
        scores["empowerment"] = min(10.0, scores["empowerment"] + 3.0)
    
    # æ ¹æ®æŸ¥è¯¢æ¨¡å¼è°ƒæ•´åˆ†æ•°
    mode_bonus = {
        "mix": 1.2,
        "hybrid": 1.1,
        "global": 1.0,
        "local": 0.9,
        "naive": 0.8
    }
    
    for key in scores:
        scores[key] *= mode_bonus.get(mode, 1.0)
        scores[key] = min(10.0, scores[key])
    
    # è®¡ç®—åŠ æƒæ€»åˆ†
    total_score = (
        scores["comprehensiveness"] * SCORING_CONFIG["comprehensiveness_weight"] +
        scores["diversity"] * SCORING_CONFIG["diversity_weight"] +
        scores["empowerment"] * SCORING_CONFIG["empowerment_weight"]
    )
    
    return {
        "total_score": round(total_score, 2),
        "comprehensiveness_score": scores["comprehensiveness"],
        "diversity_score": scores["diversity"],
        "empowerment_score": scores["empowerment"],
        "feedback": [
            f"Comprehensiveness: {scores['comprehensiveness']:.1f}/10",
            f"Diversity: {scores['diversity']:.1f}/10",
            f"Empowerment: {scores['empowerment']:.1f}/10"
        ]
    }

def query_with_best_mode(question, language):
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å¼çš„æŸ¥è¯¢åŠŸèƒ½"""
    modes = ["naive", "local", "global", "hybrid", "mix"]
    best_result = None
    best_score = 0
    best_mode = "mix"
    mode_results = {}
    
    # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
    system_prompt = generate_system_prompt(question, language)
    
    # ä¸´æ—¶æ›¿æ¢ llm_model_func ä»¥ä¼ é€’ç³»ç»Ÿæç¤º
    original_llm_func = rag.llm_model_func
    
    async def llm_with_system_prompt(prompt, system_prompt=None, history_messages=[], **kwargs):
        return await original_llm_func(prompt, system_prompt=system_prompt, history_messages=history_messages, **kwargs)
    
    rag.llm_model_func = llm_with_system_prompt
    
    try:
        # æµ‹è¯•æ‰€æœ‰æ¨¡å¼
        for mode in modes:
            try:
                # æ‰§è¡ŒæŸ¥è¯¢
                response = rag.query(question, param=QueryParam(mode=mode, top_k=10))
                
                # è®¡ç®—tokenå’Œæˆæœ¬
                input_tokens = calculate_tokens(question)
                output_tokens = calculate_tokens(response)
                cost_info = calculate_cost(input_tokens, output_tokens)
                
                # è¯„åˆ†
                score_info = score_response(question, response, mode)
                
                # è®°å½•ç»“æœ
                result = {
                    "response": response,
                    "mode": mode,
                    "score": score_info["total_score"],
                    "cost": cost_info,
                    "tokens": {
                        "input": input_tokens,
                        "output": output_tokens
                    },
                    "score_details": score_info
                }
                
                mode_results[mode] = result
                
                # æ›´æ–°æœ€ä½³ç»“æœ
                if score_info["total_score"] > best_score:
                    best_score = score_info["total_score"]
                    best_result = result
                    best_mode = mode
                    
            except Exception as e:
                print(f"æ¨¡å¼ {mode} æŸ¥è¯¢å¤±è´¥: {e}")
                continue
        
        # æ¢å¤åŸå§‹ llm_model_func
        rag.llm_model_func = original_llm_func
        
        if best_result:
            # æ›´æ–°æˆæœ¬ç»Ÿè®¡ - åªè®¡ç®—æœ€ä½³æ¨¡å¼çš„æˆæœ¬
            cost_stats["total_input_tokens"] += best_result["tokens"]["input"]
            cost_stats["total_output_tokens"] += best_result["tokens"]["output"]
            cost_stats["total_cost"] += best_result["cost"]["total_cost"]
            
            # è®°å½•åˆ°token_usage_historyåˆ—è¡¨ï¼ˆç”¨äºå‰ç«¯å›¾è¡¨æ˜¾ç¤ºï¼‰
            token_history_record = {
                "timestamp": datetime.now().isoformat(),
                "input_tokens": best_result["tokens"]["input"],
                "output_tokens": best_result["tokens"]["output"],
                "total_tokens": best_result["tokens"]["input"] + best_result["tokens"]["output"],
                "cost": best_result["cost"]["total_cost"]
            }
            token_usage_history.append(token_history_record)
            
            # è®°å½•æŸ¥è¯¢å†å²
            query_record = {
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "question": question,
                "response": best_result["response"],
                "mode": best_mode,
                "language": language,
                "score": best_result["score"],
                "cost": best_result["cost"]["total_cost"],
                "input_tokens": best_result["tokens"]["input"],
                "output_tokens": best_result["tokens"]["output"],
                "all_mode_results": mode_results
            }
            query_history.append(query_record)
            
            return {
                "response": best_result["response"],
                "timestamp": query_record["timestamp"],
                "language": language,
                "mode": best_mode,
                "score": best_result["score_details"],
                "cost": best_result["cost"],
                "tokens": best_result["tokens"],
                "mode_results": mode_results,
                "best_mode": best_mode
            }
        else:
            return {"error": "æ‰€æœ‰æ¨¡å¼éƒ½æŸ¥è¯¢å¤±è´¥"}
            
    except Exception as e:
        # æ¢å¤åŸå§‹ llm_model_func
        rag.llm_model_func = original_llm_func
        return {"error": f"æŸ¥è¯¢å‡ºé”™: {str(e)}"}

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
@require_api_key
def chat():
    try:
        data = request.get_json()
        question = data.get('message', '')
        mode = data.get('mode', 'best')  # é»˜è®¤ä½¿ç”¨æœ€ä½³æ¨¡å¼
        
        # è¾“å…¥éªŒè¯
        if not validate_input(question):
            log_security_event("INVALID_INPUT", f"Invalid input from {request.remote_addr}: {question[:50]}")
            return jsonify({'error': 'Invalid input detected'}), 400
        
        if not question:
            return jsonify({'error': 'è¯·è¾“å…¥é—®é¢˜'})
        
        # æ£€æµ‹è¯­è¨€
        language = detect_language(question)
        
        if mode == 'best':
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å¼
            result = query_with_best_mode(question, language)
        else:
            # ä½¿ç”¨æŒ‡å®šæ¨¡å¼
            system_prompt = generate_system_prompt(question, language)
            
            # ä¸´æ—¶æ›¿æ¢ llm_model_func ä»¥ä¼ é€’ç³»ç»Ÿæç¤º
            original_llm_func = rag.llm_model_func
            
            async def llm_with_system_prompt(prompt, system_prompt=None, history_messages=[], **kwargs):
                return await original_llm_func(prompt, system_prompt=system_prompt, history_messages=history_messages, **kwargs)
            
            rag.llm_model_func = llm_with_system_prompt
            
            try:
                # ä½¿ç”¨åŒæ­¥æŸ¥è¯¢æ–¹æ³•
                response = rag.query(question, param=QueryParam(mode=mode, top_k=10))
                
                # è®¡ç®—tokenå’Œæˆæœ¬
                input_tokens = calculate_tokens(question)
                output_tokens = calculate_tokens(response)
                cost_info = calculate_cost(input_tokens, output_tokens)
                
                # æ›´æ–°æˆæœ¬ç»Ÿè®¡
                cost_stats["total_input_tokens"] += input_tokens
                cost_stats["total_output_tokens"] += output_tokens
                cost_stats["total_cost"] += cost_info["total_cost"]
                
                # è®°å½•åˆ°token_usage_historyåˆ—è¡¨ï¼ˆç”¨äºå‰ç«¯å›¾è¡¨æ˜¾ç¤ºï¼‰
                token_history_record = {
                    "timestamp": datetime.now().isoformat(),
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": input_tokens + output_tokens,
                    "cost": cost_info["total_cost"]
                }
                token_usage_history.append(token_history_record)
                
                # è¯„åˆ†
                score_info = score_response(question, response, mode)
                
                # è®°å½•æŸ¥è¯¢å†å²
                query_record = {
                    "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "question": question,
                    "response": response,
                    "mode": mode,
                    "language": language,
                    "score": score_info["total_score"],
                    "cost": cost_info["total_cost"],
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens
                }
                query_history.append(query_record)
                
                result = {
                    'response': response,
                    'timestamp': query_record["timestamp"],
                    'language': language,
                    'mode': mode,
                    'score': score_info,
                    'cost': cost_info,
                    'tokens': {
                        'input': input_tokens,
                        'output': output_tokens
                    }
                }
            finally:
                # æ¢å¤åŸå§‹ llm_model_func
                rag.llm_model_func = original_llm_func
        
        if 'error' in result:
            return jsonify({'success': False, 'error': result['error']})
        else:
            return jsonify({
                'success': True,
                'response': result['response'],
                'timestamp': result.get('timestamp', ''),
                'language': result.get('language', 'english'),
                'mode_used': result.get('mode', result.get('best_mode', 'unknown')),
                'processing_time': 0,  # å¯ä»¥åç»­æ·»åŠ å®é™…å¤„ç†æ—¶é—´
                'score': result.get('score', {}),
                'cost': result.get('cost', {}),
                'tokens': result.get('tokens', {})
            })
        
    except Exception as e:
        return jsonify({'error': f'é”™è¯¯ï¼š{str(e)}'})

@app.route('/stats')
def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    return jsonify({
        'cost_stats': cost_stats,
        'query_history': query_history[-10:],  # æœ€è¿‘10æ¡è®°å½•
        'total_queries': len(query_history)
    })

@app.route('/token_usage')
def get_token_usage():
    """è·å–tokenä½¿ç”¨æƒ…å†µ"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        days = request.args.get('days', 7, type=int)
        summary_only = request.args.get('summary', 'false').lower() == 'true'
        
        # è®¡ç®—æ€»ä½¿ç”¨æƒ…å†µ
        total_input_tokens = sum(record.get('input_tokens', 0) for record in token_usage_history)
        total_output_tokens = sum(record.get('output_tokens', 0) for record in token_usage_history)
        total_cost = sum(record.get('cost', 0) for record in token_usage_history)
        
        total_usage = {
            "total_tokens": total_input_tokens + total_output_tokens,
            "input_tokens": total_input_tokens,
            "output_tokens": total_output_tokens,
            "estimated_cost": total_cost
        }
        
        # æŒ‰æ¨¡å‹åˆ†ç»„ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºå•ä¸€æ¨¡å‹ï¼‰
        model_usage = {
            "gpt-4o-mini": {
                "total_tokens": total_usage["total_tokens"],
                "input_tokens": total_input_tokens,
                "output_tokens": total_output_tokens,
                "estimated_cost": total_cost,
                "requests": len(token_usage_history)
            }
        }
        
        response_data = {
            "total": total_usage,
            "models": model_usage,
            "last_updated": datetime.now().isoformat()
        }
        
        # å¦‚æœä¸åªæ˜¯æ‘˜è¦ï¼Œæ·»åŠ æ¯æ—¥ä½¿ç”¨æƒ…å†µ
        if not summary_only:
            # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
            from collections import defaultdict
            daily_stats = defaultdict(lambda: {"total_tokens": 0, "input_tokens": 0, "output_tokens": 0, "cost": 0, "requests": 0})
            
            for record in token_usage_history:
                date = record["timestamp"][:10]  # å–æ—¥æœŸéƒ¨åˆ†
                daily_stats[date]["total_tokens"] += record.get("total_tokens", 0)
                daily_stats[date]["input_tokens"] += record.get("input_tokens", 0)
                daily_stats[date]["output_tokens"] += record.get("output_tokens", 0)
                daily_stats[date]["cost"] += record.get("cost", 0)
                daily_stats[date]["requests"] += 1
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            recent_daily = []
            for date, stats in sorted(daily_stats.items(), reverse=True)[:days]:
                recent_daily.append({
                    "date": date,
                    "total_tokens": stats["total_tokens"],
                    "input_tokens": stats["input_tokens"],
                    "output_tokens": stats["output_tokens"],
                    "estimated_cost": stats["cost"],
                    "requests": stats["requests"]
                })
            
            response_data["recent_daily"] = recent_daily
        
        return jsonify({
            "success": True,
            "data": response_data,
            "history": token_usage_history
        })
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/api/token_usage')
def get_token_usage_history():
    """è·å–tokenä½¿ç”¨å†å²è®°å½•ï¼ˆç”¨äºå‰ç«¯å›¾è¡¨ï¼‰"""
    try:
        # è¿”å›token_usage_historyåˆ—è¡¨ï¼Œæ ¼å¼ç¬¦åˆå‰ç«¯å›¾è¡¨éœ€æ±‚
        return jsonify(token_usage_history)
    except Exception as e:
        return jsonify({
            "error": str(e)
        }), 500

@app.route('/test_modes')
def test_modes():
    """æµ‹è¯•ä¸åŒæŸ¥è¯¢æ¨¡å¼"""
    test_question = "What are the key stakeholder engagement strategies in the Scarborough project?"
    modes = ["naive", "local", "global", "hybrid", "mix"]
    results = []
    
    for mode in modes:
        try:
            response = rag.query(test_question, param=QueryParam(mode=mode, top_k=10))
            score_info = score_response(test_question, response, mode)
            results.append({
                'mode': mode,
                'response': response[:200] + "..." if len(response) > 200 else response,
                'score': score_info["total_score"],
                'feedback': score_info["feedback"]
            })
        except Exception as e:
            results.append({
                'mode': mode,
                'error': str(e)
            })
    
    return jsonify({'test_results': results})

@app.route('/health')
def health():
    return jsonify({
        'status': 'healthy', 
        'rag_initialized': rag is not None,
        'total_queries': len(query_history),
        'total_cost': cost_stats["total_cost"]
    })

if __name__ == '__main__':
    print("ğŸš€ åˆå§‹åŒ– Stakeholder Management Chatbot Web ç•Œé¢...")
    initialize_rag()
    print("ğŸŒ å¯åŠ¨ Web æœåŠ¡å™¨...")
    
    # è·å–ç«¯å£å·ï¼ŒRenderä¼šè‡ªåŠ¨è®¾ç½®PORTç¯å¢ƒå˜é‡
    port = int(os.environ.get('PORT', 8081))
    app.run(host='0.0.0.0', port=port, debug=False) 